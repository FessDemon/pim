{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3. Полиномиальная регрессия. Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные из файлов `ml_lab1_train.txt` и `ml_lab1_test.txt` (первая лабораторная работа).\n",
    "\n",
    "Для $k = 1,2,3,\\ldots,10$ найдите полином ${f}_k$ степени $k$, наилучшим образом приближающий неизвестную зависимость.\n",
    "\n",
    "Выведите коэффициенты полиномов и нарисуйте их графики на одном чертеже вместе с точками данных $(x_i, y_i)$ (возможно, чертеж стоит сделать побольше; это делается командой `plt.figure(figsize=(width, height))`).\n",
    "\n",
    "Для каждого из полиномов найдите среднеквадратическую ошибку $MSE$ и коэффициент детерминации $R^2$ на обучающих данных и на тестовых данных. Постройте графики зависимости $MSE$ на обучающих и тестовых данных в зависимости от степени полинома (должно быть три графика: один для обучающих данных, второй – для тестовых, третий – для обоих видов данных вместе).\n",
    "\n",
    "Сделайте вывод, что происходит с ошибкой на обучающих и тестовых данных с увеличением степени полинома."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/_core/__init__.py:23\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/_core/multiarray.py:10\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/_core/overrides.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[1;32m     11\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core._multiarray_umath'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/__init__.py:114\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_config\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/__config__.py:4\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     __cpu_features__,\n\u001b[1;32m      6\u001b[0m     __cpu_baseline__,\n\u001b[1;32m      7\u001b[0m     __cpu_dispatch__,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_config\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/_core/__init__.py:49\u001b[0m\n\u001b[1;32m     26\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[1;32m     48\u001b[0m         __version__, exc)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.12 from \"/home/demon/anaconda3/bin/python\"\n  * The NumPy version is: \"2.2.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: No module named 'numpy._core._multiarray_umath'\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msla\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/numpy/__init__.py:119\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    116\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124m    your python interpreter from there.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _core\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m     False_, ScalarType, True_,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mabs\u001b[39m, absolute, acos, acosh, add, \u001b[38;5;28mall\u001b[39m, allclose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     vecmat, void, vstack, where, zeros, zeros_like\n\u001b[1;32m    170\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sla\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statistics \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data_train = np.loadtxt('ml_lab1_train.txt', delimiter=',')\n",
    "data_test = np.loadtxt('ml_lab1_test.txt', delimiter=',')\n",
    "# Разделение данных и целевых значений для обучающих данных\n",
    "X_train = data_train[:,0]\n",
    "y_train = data_train[:,1]\n",
    "# Разделение данных и целевых значений для тестовых данных\n",
    "X_test = data_test[:,0]\n",
    "y_test = data_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальная регрессия на основе нормального уравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нахождения значений по линейной функции с предсказанными весовыми коэффициентами\n",
    "def f_reg(X, w):\n",
    "    X = np.column_stack([np.ones(X.shape[0]).T, X])\n",
    "    y_predict = np.dot(w, X.T)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нахождения вектора весов на основе нормального уравнения (a = 0 без регуляризации, а != 0 с регуляризацией\n",
    "def get_weight(X, y, a=0):\n",
    "    X = np.column_stack([np.ones(X.shape[0]).T, X])    \n",
    "    w = np.dot(np.linalg.inv(np.dot(X.T, X) + a * np.eye(X.shape[1])), np.dot(X.T, y))\n",
    "    return w  # Возвращаем вектор весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание полиномиальных признаков\n",
    "def create_PolynomialFeatures(degree, X_train, X_test):\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_poly_train = poly.fit_transform(X_train.reshape(-1,1))\n",
    "    X_poly_test = poly.fit_transform(X_test.reshape(-1,1))\n",
    "    return X_poly_train, X_poly_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полиномиальная регрессия на основе нормального уравнения\n",
    "def poli_reg(X_train, X_test, y_train, degree):\n",
    "    X_poly_train, X_poly_test = create_PolynomialFeatures(i + min_degree, X_train, X_test)    \n",
    "    w = get_weight(X_poly_train, y_train)\n",
    "    return X_poly_train, X_poly_test, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = []\n",
    "r2_train = []\n",
    "mse_test = []\n",
    "r2_test = []\n",
    "arr_w = []\n",
    "arr_y_predict_train = []\n",
    "arr_y_predict_test = []\n",
    "\n",
    "print(X_train.shape[0])\n",
    "count = 10\n",
    "min_degree = 1\n",
    "for i in range(count):\n",
    "    X_poly_train, X_poly_test, w = poli_reg(X_train, X_test, y_train, i + min_degree)\n",
    "    arr_w.append(w)\n",
    "    \n",
    "    y_predict_train = f_reg(X_poly_train, w)\n",
    "    arr_y_predict_train.append(y_predict_train)\n",
    "    y_predict_test = f_reg(X_poly_test, w)\n",
    "    arr_y_predict_test.append(y_predict_test)\n",
    "    \n",
    "    mse_train.append(mean_squared_error(y_train, y_predict_train))\n",
    "    r2_train.append(r2_score(y_train, y_predict_train))\n",
    "    \n",
    "    mse_test.append(mean_squared_error(y_test, y_predict_test))\n",
    "    r2_test.append(r2_score(y_test, y_predict_test))\n",
    "    print('Степень = ', i + min_degree, '\\tw = ', w, '\\n\\nMSE_train = ', mse_train[i], '\\tr2_train = ', r2_train[i], '\\nMSE_test = ', mse_test[i], '\\tr2_test = ', r2_test[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 1. Полиномиальная регрессия (на основе нормального уравнения). Построение полученных полиномов\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "width = 15\n",
    "height = 10\n",
    "m = 0.1\n",
    "\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.title('Полиномиальная регрессия')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(X_train, y_train, 40, 'g', 'o', alpha=0.8, label='train')\n",
    "plt.scatter(X_test, y_test, 40, 'b', 'x', alpha=0.8, label='test')\n",
    "\n",
    "for i in range(10):\n",
    "    plt.plot(X_train, arr_y_predict_train[i], color = colors[i], label='Полином '+str(i + min_degree)+' степени')\n",
    "    plt.legend(loc = 'best', prop = {'size': 10})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 1. Построение графиков зависимости функции ошибки от степени полинома\n",
    "width = 15\n",
    "height = 5\n",
    "m = 1\n",
    "plt.figure(figsize=(width, height))\n",
    "x = [i+1 for i in range(count)]\n",
    "# 1-ый график - ошибка на обучающем наборе\n",
    "sp = plt.subplot(131)\n",
    "plt.plot(x, mse_train, color = 'green', label='train')\n",
    "plt.title('MSE_train')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "# 2-ой график - ошибка на тестовом наборе\n",
    "sp = plt.subplot(132)\n",
    "plt.plot(x, mse_test, color = 'blue', label='test')\n",
    "plt.title('MSE_test')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "# 3-ий график - графики ошибок на обучающем и тестовом наборах\n",
    "sp = plt.subplot(133)\n",
    "plt.plot(x, mse_train, color = 'green', label='train')\n",
    "plt.plot(x, mse_test, color = 'blue', label='test')\n",
    "plt.legend(loc = 'best', prop = {'size': 10})\n",
    "plt.title('MSE_train & test')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degree = mse_test.index(min(mse_test)) + min_degree\n",
    "print('Вывод: наилучший результат достигается при', best_degree, 'степени. При дальнейшем увеличении степени полинома происходит переобучение.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ***Выполнение задания 1 на основе градиентного спуска***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление градиента\n",
    "def gradient(X, y, w):\n",
    "    return 2 * np.dot(X.T, np.dot(X, w) - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обобщенный вариант функции градиентного спуска\n",
    "\n",
    "# По умолчанию (learn_rate=0.5, n_iter=100, eps=1e-06, batch_size=len(y), stochastic=False) - классический градиентный спуск\n",
    "# При задании stochastic = True и batch_size < len(y) - стохастический по мини-батчам\n",
    "\n",
    "def gradient_descent_all(X, y, w, learn_rate=0.5, n_iter=100, eps=1e-06, batch_size=None, stochastic=False):\n",
    "    if stochastic:\n",
    "        n_obs = X.shape[0] # Количество наблюдений (значений)\n",
    "        xy = np.c_[X.reshape(n_obs, 1), y.reshape(n_obs, 1)] # Объединение массивов x и y в один\n",
    "    \n",
    "    k_iter = 0\n",
    "    n = learn_rate\n",
    "    for _ in range(n_iter):\n",
    "        if stochastic:\n",
    "            rng.shuffle(xy) # Перемешивание x и y\n",
    "            start = np.random.randint(n_obs-batch_size) # Выбор случайного значения\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = np.array(xy[start:stop, :-1].ravel().tolist()), np.array(xy[start:stop, -1:].ravel().tolist()) # Формирование мини-батча и разделение x и y\n",
    "        else:\n",
    "            x_batch = X\n",
    "            y_batch = y\n",
    "        \n",
    "        x_batch = np.column_stack([np.ones(x_batch.shape[0]).T, x_batch])            \n",
    "        \n",
    "        diff = learn_rate * gradient(x_batch, y_batch, w)\n",
    "        w -= diff\n",
    "        \n",
    "        k_iter += 1    \n",
    "        if np.all(np.abs(diff) <= eps):\n",
    "            break     \n",
    "        learn_rate -= n / n_iter\n",
    "    return w, k_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Градиентный спуск\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "mse_train = []\n",
    "r2_train = []\n",
    "mse_test = []\n",
    "r2_test = []\n",
    "arr_w = []\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "width = 10\n",
    "height = 7\n",
    "m = 0.1\n",
    "plt.figure(figsize=(width, height))\n",
    "x_min = min(X_train.min(), X_test.min())\n",
    "x_max = max(X_train.max(), X_test.max())\n",
    "y_min = min(y_train.min(), y_test.min())\n",
    "y_max = max(y_train.max(), y_test.max())\n",
    "plt.scatter(X_train, y_train, 40, 'g', 'o', alpha=0.8, label='train')\n",
    "plt.scatter(X_test, y_test, 40, 'b', 'x', alpha=0.8, label='test')\n",
    "plt.xlim(x_min - m, x_max + m)\n",
    "plt.ylim(y_min - m, y_max + m)\n",
    "plt.legend(loc = 'best', prop = {'size': 10})\n",
    "\n",
    "count = 10\n",
    "min_degree = 1\n",
    "for i in range(count):\n",
    "    # Создание полиномиальных признаков\n",
    "    poly = PolynomialFeatures(i + min_degree, include_bias=False)\n",
    "    X_poly_train = poly.fit_transform(X_train.reshape(-1,1))\n",
    "    X_poly_test = poly.fit_transform(X_test.reshape(-1,1))\n",
    "    w_0 = np.ones(X_poly_train.shape[1] + 1)\n",
    "    w, k = gradient_descent_all(X_poly_train, y_train, w_0, learn_rate=1, n_iter = 120)\n",
    "    #w, k = gradient_descent(X_poly_train, y_train, w, learn_rate=0.5, n_iter=100, eps=1e-06, batch_size=len(y), stochastic=False)\n",
    "    arr_w.append(w)\n",
    "    y_predict_train = f_reg(X_poly_train, w)\n",
    "    plt.plot(X_train, y_predict_train, color = colors[i], label=str(i + min_degree))\n",
    "    plt.legend(loc = 'best', prop = {'size': 10})\n",
    "    mse_train.append(mean_squared_error(y_train, y_predict_train))\n",
    "    r2_train.append(r2_score(y_train, y_predict_train))\n",
    "    y_predict_test = f_reg(X_poly_test, w)\n",
    "    mse_test.append(mean_squared_error(y_test, y_predict_test))\n",
    "    r2_test.append(r2_score(y_test, y_predict_test))\n",
    "    print('Степень = ', i + min_degree, 'w = ', w, ' Количество итераций = ', k, '\\nMSE_train = ', mse_train[i], 'r2_train = ', r2_train[i], '\\nMSE_test = ', mse_test[i], 'r2_test = ', r2_test[i],'\\n')\n",
    "    #val_err(X_poly_train, y, mse_train, r2_train)\n",
    "\n",
    "plt.title('Полиномиальная регрессия')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графиков функции ошибки\n",
    "width = 10\n",
    "height = 15\n",
    "m = 1\n",
    "plt.figure(figsize=(width, height))\n",
    "x = [i+1 for i in range(count)]\n",
    "#subplot train\n",
    "sp = plt.subplot(311)\n",
    "plt.plot(x, mse_train, color = 'green', label='train')\n",
    "plt.title('MSE_train')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "#subplot test\n",
    "sp = plt.subplot(312)\n",
    "plt.plot(x, mse_test, color = 'blue', label='test')\n",
    "plt.title('MSE_test')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "#subplot train and test\n",
    "sp = plt.subplot(313)\n",
    "plt.plot(x, mse_train, color = 'green', label='train')\n",
    "plt.plot(x, mse_test, color = 'blue', label='test')\n",
    "plt.legend(loc = 'best', prop = {'size': 10})\n",
    "plt.title('MSE')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Вывод: наилучший результат достигается при', mse_test.index(min(mse_test)) + 1, 'степени. При дальнейшем увеличении степени полинома результат практически не меняется.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что задачу линейной регрессии (нахождение вектора коэффициентов $\\overrightarrow{w}_{opt}$) можно решить при помощи нормального уравнения:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$\n",
    "\n",
    "Если строки или столбцы матрицы $X$ линейно зависимы, то матрица $X^TX$ вырожденна и задача не может быть решена с помощью нормального уравнения (придется использовать, например, градиентный спуск). На практике различные признаки редко бывают *в точности* линейно зависимы, однако во многих ситуациях они скоррелированы и становятся \"почти\" линейно зависимыми. Таковы, к примеру, зарплата человека, его уровень образования, цена машины и суммарная площадь недвижимости, которой он владеет. В этом случае матрица $X^TX$ будет близка к вырожденной, и это приводит к численной неустойчивости и плохому качеству решений; как следствие, будет иметь место *переобучение* (overfitting). Один из симптомов этой проблемы – необычно большие по модулю компоненты вектора $\\overrightarrow{w}_{opt}$.\n",
    "\n",
    "Один из способов борьбы с переобучением – **регуляризация**. Сейчас мы рассмотрим одну из её разновидностей – **L2-регуляризацию**. Идея в том, чтобы подправить матрицу $X^TX$, сделав её \"получше\". Например, это можно сделать, заменив её на $(X^TX + \\alpha I)$, где $\\alpha$ – параметр регуляризации, $I$ – единичная матрица. Пожертвовав точностью на обучающей выборке, мы тем не менее получаем численно более стабильное псевдорешение:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = (X^TX + \\alpha I)^{-1}X^T\\overrightarrow{y}$$\n",
    "\n",
    "и снижаем эффект переобучения. Параметр $\\alpha$ нужно подбирать, и каких-то универсальных способов это делать нет, но зачастую можно его подобрать таким, чтобы ошибка на тестовой выборке падала. \n",
    "\n",
    "Теперь давайте вспомним первую задачу. Если вы её сделали, то помните, что ошибка аппроксимации полиномом шестой степени довольно высокая. Убедитесь, что, используя регуляризацию с хорошо подобранным параметром $\\alpha$, ошибку на тестовой выборке можно сделать не больше, чем для полинома оптимальной степени в модели без регуляризации. Для этого $\\alpha$ сравните $\\det(X^TX)$ и $\\det(X^TX + \\alpha I)$.\n",
    "\n",
    "Изобразите на графике три полинома: полином оптимальной степени без регуляризации, полином шестой степени без регуляризации и полином шестой степени с регуляризацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выше описана функция нахождения вектора весов на основе нормального уравнения с L2-регуляризацией (a != 0)\n",
    "# get_weight(X, y, a=0):\n",
    "#    X = np.column_stack([np.ones(X.shape[0]).T, X])\n",
    "#    w = np.dot(np.linalg.inv(np.dot(X.T, X) + a * np.eye(X.shape[1])), np.dot(X.T, y))\n",
    "#    return w  # Возвращаем вектор весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 1, 2, 3, 5, 8, 10, 20, 50, 100]\n",
    "# Создание полиномиальных признаков\n",
    "X_poly_train, X_poly_test = create_PolynomialFeatures(6, X_train, X_test)\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "arr_w = []\n",
    "for a in alpha_values:\n",
    "    w = get_weight(X_poly_train, y_train, a)\n",
    "    arr_w.append(w)\n",
    "    y_predict_train = f_reg(X_poly_train, w)\n",
    "    mse_train.append(mean_squared_error(y_train, y_predict_train))\n",
    "    y_predict_test = f_reg(X_poly_test, w)\n",
    "    mse_test.append(mean_squared_error(y_test, y_predict_test))\n",
    "    print('a =', a, '\\tw = ', w, '\\nMSE_train = ', mse_train[len(mse_train)-1], '\\nMSE_test = ', mse_test[len(mse_train)-1],'\\n')\n",
    "print('Вывод: наилучший результат достигается при a =', alpha_values[mse_test.index(min(mse_test))])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним определители det(X.T*X) и det(X.T*X + alpha*I)\n",
    "X = np.column_stack([np.ones(X_poly_train.shape[0]).T, X_poly_train])\n",
    "print('det(X.T*X) =', np.linalg.det(np.dot(X.T, X)))\n",
    "print('det(X.T*X + alpha*I) =', np.linalg.det(np.dot(X.T, X) + a * np.eye(X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train, X_poly_test = create_PolynomialFeatures(6, X_train, X_test)\n",
    "\n",
    "w6 = get_weight(X_poly_train, y_train, a = 0) # Полином 6-ой степени без регуляризации\n",
    "w6_a = get_weight(X_poly_train, y_train, a = 1) # Полином 6-ой степени с регуляризацией (a = 1)\n",
    "y6_predict_train = f_reg(X_poly_train, w6)\n",
    "y6_a_predict_train = f_reg(X_poly_train, w6_a)\n",
    "\n",
    "X_poly_train, X_poly_test = create_PolynomialFeatures(best_degree, X_train, X_test)\n",
    "w_best = get_weight(X_poly_train, y_train, a = 0) # Полином 5-ей степени без регуляризации (лучшая степень полинома)\n",
    "y_best_predict_train = f_reg(X_poly_train, w_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графиков\n",
    "width = 15\n",
    "height = 10\n",
    "m = 0.1\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.scatter(X_train, y_train, 40, 'g', 'o', alpha=0.8, label='train')\n",
    "plt.scatter(X_test, y_test, 40, 'b', 'x', alpha=0.8, label='test')\n",
    "plt.plot(X_train, y_best_predict_train, 'c', label='Полином ' + str(best_degree) + ' степени без регуляризации')\n",
    "plt.plot(X_train, y6_predict_train, 'y', label='Полином 6 степени без регуляризации')\n",
    "plt.plot(X_train, y6_a_predict_train, 'r', label='Полином 6 степени с регуляризацией')\n",
    "plt.legend(loc = 'best', prop = {'size': 10})\n",
    "plt.title('Полиномиальная регрессия')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Линейная регрессия в scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте файлы ``flats_moscow.txt`` и ``flats_moscow_description.pdf``. В первом из них содержатся данные о квартирах в Москве. Каждая строка содержит шесть характеристик некоторой квартиры, разделённые знаками табуляции; в первой строке записаны кодовые названия характеристик. Во втором файле приведены краткие описания признаков. Вашей задачей будет построить с помощью линейной регрессии зависимость между ценой квартиры и остальными доступными параметрами.\n",
    "\n",
    "Построив несколько моделей, нужно сравнить их качество. Проверять это на той же выборке, на которой вы обучались, бессмысленно и даже вредно (вспомните пример с полиномами: как хорошо падала ошибка на обучающей выборке с ростом степени и как быстро росла ошибка на тестовых данных!). Поэтому вам нужно будет разделить выборку на обучающую и тестовую. Делать это лучше случайным образом (ведь вы не знаете, как создатели набора данных упорядочили объекты); рекомендуем вам для этого функцию `sklearn.model_selection.train_test_split`.\n",
    "\n",
    "Постройте следующие модели линейной регрессии по методу наименьших квадратов:\n",
    "\n",
    "1. на основе собственной функции (нормальное уравнение);\n",
    "\n",
    "1. с L2-регуляризацией на основе собственной функции – параметр регуляризации нужно подобрать;\n",
    "\n",
    "1. собственную реализацию полиномиальной регрессии – степень полинома нужно подобрать;\n",
    "\n",
    "1. на основе функции `LinearRegression` из `scikit-learn`;\n",
    "\n",
    "1. с L2-регуляризацией на основе функции `Ridge` из `scikit-learn` – параметр регуляризации нужно подобрать;\n",
    "\n",
    "1. с L1-регуляризацией на основе функции `Lasso` из `scikit-learn` – параметр регуляризации нужно подобрать.\n",
    "\n",
    "Выведите и сравните регрессионные коэффициенты для всех функций. Какой смысл имеют их знаки? Согласуются ли они с вашими представлениями о жизни?\n",
    "\n",
    "Оцените качество решения задачи, выведя среднеквадратическую ошибку на обучающих и тестовых данных для всех функций. Эти ошибки лучше свести в таблицу (например, используя `pandas.DataFrame`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flats_moscow.txt', sep='\\t')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price']) # В признаки занесем все без порядкового номера и стоимости\n",
    "y = df['price'] # В значения занесем стоимость\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, random_state=42) # Случайное разделение на train и test в отношении 2:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нахождения значений по линейной функции с предсказанными весовыми коэффициентами\n",
    "def f_reg(X, w):\n",
    "    X = np.column_stack([np.ones(X.shape[0]).T, X])\n",
    "    y_predict = np.dot(w, X.T)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_err(X_train, y_train, X_test, y_test, w, df_mse, i, s):\n",
    "    y_predict_train = f_reg(X_train, w)\n",
    "    mse_train = mean_squared_error(y_train, y_predict_train)\n",
    "    y_predict_test = f_reg(X_test, w)\n",
    "    mse_test = mean_squared_error(y_test, y_predict_test)\n",
    "    df_mse.loc[i] = [s, mse_train, mse_test]\n",
    "    return df_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Полиномиальная регрессия на основе нормального уравнения\n",
    "def poli_reg1(X_train, X_test, y_train, degree):\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.fit_transform(X_test)    \n",
    "    w_0 = np.ones(X_poly_train.shape[1] + 1)\n",
    "    w = get_weight(X_poly_train, y_train)\n",
    "    return X_poly_train, X_poly_test, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "df_mse = pd.DataFrame({'model': [], 'mse_train': [], 'mse_test': []}) # Датафрейм для сравнения качества решения задачи\n",
    "\n",
    "print('\\n1. На основе собственной функции (нормальное уравнение)')\n",
    "w = get_weight(X_train, y_train)\n",
    "print('w = ', w)\n",
    "i += 1\n",
    "df_mse = f_err(X_train, y_train, X_test, y_test, w, df_mse, i, 'На основе собственной функции (нормальное уравнение)')\n",
    "\n",
    "print('\\n2. L2-регуляризация на основе собственной функции')\n",
    "alpha_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "arr_w = []\n",
    "for a in alpha_values:\n",
    "    w = get_weight(X_train, y_train, a)\n",
    "    arr_w.append(w)\n",
    "    y_predict_train = f_reg(X_train, w)\n",
    "    mse_train.append(mean_squared_error(y_train, y_predict_train))\n",
    "    y_predict_test = f_reg(X_test, w)\n",
    "    mse_test.append(mean_squared_error(y_test, y_predict_test))\n",
    "w = arr_w[mse_test.index(min(mse_test))]\n",
    "alpha = alpha_values[mse_test.index(min(mse_test))]\n",
    "print('alpha =', alpha, '\\tw = ', w)\n",
    "i += 1\n",
    "df_mse = f_err(X_train, y_train, X_test, y_test, w, df_mse, i, 'L2-регуляризация на основе собственной функции')\n",
    "\n",
    "print('\\n3. Cобственная реализация полиномиальной регрессии')\n",
    "X_tr = np.array(X_train)\n",
    "X_te = np.array(X_test)\n",
    "y_tr = np.array(y_train)\n",
    "y_te = np.array(y_test)\n",
    "count = 10\n",
    "min_degree = 1\n",
    "best_mse_train = 1e+10\n",
    "best_mse_test = 1e+10\n",
    "best_w = []\n",
    "best_X_train = []\n",
    "best_X_test = []\n",
    "\n",
    "for d in range(count):\n",
    "    X_poly_train, X_poly_test, w = poli_reg1(np.array(X_train), np.array(X_test), np.array(y_train), d + min_degree)\n",
    "    \n",
    "    y_predict_train = f_reg(X_poly_train, w)\n",
    "    y_predict_test = f_reg(X_poly_test, w)\n",
    "    \n",
    "    mse_tr = mean_squared_error(y_tr, y_predict_train)\n",
    "    #r2_train = r2_score(y_tr, y_predict_train)\n",
    "            \n",
    "    mse_te = mean_squared_error(y_te, y_predict_test)\n",
    "    # r2_test = r2_score(y_te, y_predict_test))\n",
    "    \n",
    "    if mse_te < best_mse_test:\n",
    "        best_mse_train = mse_tr\n",
    "        best_mse_test = mse_te\n",
    "        best_w = w\n",
    "        best_X_train = X_poly_train\n",
    "        best_X_test = X_poly_test\n",
    "        best_degree = d+min_degree\n",
    "print('degree =', best_degree, '\\tw = ', best_w)\n",
    "i += 1\n",
    "df_mse = f_err(best_X_train, y_tr, best_X_test, y_te, best_w, df_mse, i, 'Cобственная реализация полиномиальной регрессии')\n",
    "\n",
    "print('\\n4. На основе функции LinearRegression')\n",
    "# Обучение модели\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Оценка модели\n",
    "w = np.concatenate([[model.intercept_], model.coef_])\n",
    "print('w = ', w)\n",
    "i += 1\n",
    "df_mse = f_err(X_train, y_train, X_test, y_test, w, df_mse, i, 'На основе функции LinearRegression')\n",
    "\n",
    "print('\\n5. C L2-регуляризацией на основе функции Ridge')\n",
    "best_mse = 1e+10\n",
    "best_w = []\n",
    "alpha_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "for alpha in alpha_values:\n",
    "    # Обучение модели\n",
    "    model = Ridge(alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Оценка модели\n",
    "    w = np.concatenate([[model.intercept_], model.coef_])\n",
    "    y_predict = f_reg(X_test, w)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_w = w\n",
    "        best_alpha = alpha\n",
    "print('alpha =', best_alpha,'\\tw = ', best_w)\n",
    "i += 1\n",
    "df_mse = f_err(X_train, y_train, X_test, y_test, w, df_mse, i, 'C L2-регуляризацией на основе функции Ridge')\n",
    "\n",
    "print('\\n6. C L1-регуляризацией на основе функции Lasso')\n",
    "best_mse = 1e+10\n",
    "best_w = []\n",
    "alpha_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
    "for alpha in alpha_values:\n",
    "    # Обучение модели\n",
    "    model = Lasso(alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    # Оценка модели\n",
    "    w = np.concatenate([[model.intercept_], model.coef_])\n",
    "    y_predict = f_reg(X_test, w)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_w = w\n",
    "        best_alpha = alpha\n",
    "print('alpha =', best_alpha,'\\tw = ', best_w)\n",
    "i += 1\n",
    "df_mse = f_err(X_train, y_train, X_test, y_test, w, df_mse, i, 'C L1-регуляризацией на основе функции Lasso')\n",
    "\n",
    "# Вывод ошибок на обучающем и тестовом наборах для построенных функций\n",
    "print('\\nСреднеквадратичные ошибки на обучающем и тестовом наборах для построенных функций\\n')\n",
    "print(df_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_Вывод: Знаки \"-\" у коэффициентов при dist и metrdist говорят о том, что эти параметры отрицательно сказываются на цене (чем больше расстояние, тем ниже цена). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, никто не гарантирует, что объясняемая переменная (цена квартиры) зависит от остальных характеристик именно линейно. Зависимость может быть, например, квадратичной или логарифмической; больше того, могут быть важны не только отдельные признаки, но и их комбинации. Это можно учитывать, добавляя в качестве дополнительных признаков разные функции от уже имеющихся характеристик: их квадраты, логарифмы, попарные произведения.\n",
    "\n",
    "В этом задании вам нужно постараться улучшить качество модели, добавляя дополнительные признаки (не менее трёх), являющиеся функциями от уже имеющихся. Но будьте осторожны: чрезмерное усложнение модели будет приводить к переобучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции ошибки\n",
    "def get_mse(f, X_train, y_train, X_test, y_test):\n",
    "    # Добавляем значения выбранной функции для признаков\n",
    "    f_X_train = f(X_train)\n",
    "    new_X_train = np.concatenate((X_train, f_X_train), axis = 1)\n",
    "    f_X_test = f(X_test)\n",
    "    new_X_test = np.concatenate((X_test, f_X_test), axis = 1)\n",
    "    # Нахождение коэффициентов регрессии на основе нормального уравнения\n",
    "    w = get_weight(new_X_train, y_train)\n",
    "    # Нахождение предсказанных значений и функции ошибки \n",
    "    y_predict_train = f_reg(new_X_train, w)\n",
    "    mse_train = mean_squared_error(y_train, y_predict_train)\n",
    "    y_predict_test = f_reg(new_X_test, w)\n",
    "    mse_test = mean_squared_error(y_test, y_predict_test)\n",
    "    return mse_train, mse_test\n",
    "\n",
    "# Функция добавления произведений пар признаков\n",
    "def pair_X(X):\n",
    "    X = np.array(X)\n",
    "    new_X = np.array([]).reshape(X.shape[0], 0)\n",
    "    for i in range(X.shape[1] - 1):\n",
    "        for j in range(i, X.shape[1]):\n",
    "            new_column = np.multiply(X[:,i], X[:,j])\n",
    "            new_X = np.column_stack((new_X, new_column))\n",
    "    return new_X\n",
    "\n",
    "add_funcs = {\n",
    "    'none': lambda X: np.array([]).reshape(X.shape[0], 0),\n",
    "    'square': lambda X : X ** 2,\n",
    "    'cube': lambda X : X ** 3,\n",
    "    'fourth': lambda X : X ** 4,\n",
    "    'fifth': lambda X : X ** 5,\n",
    "    'six': lambda X : X ** 6,\n",
    "    'seven': lambda X : X ** 7,\n",
    "    'eight': lambda X : X ** 8,\n",
    "    'nine': lambda X : X ** 9,\n",
    "    'tenth': lambda X : X ** 10,\n",
    "    'log': lambda X : np.log(X),\n",
    "    'xlog': lambda X : X * np.log(X),\n",
    "    'pair_X': pair_X\n",
    "}\n",
    "\n",
    "min_mse_test = 1e+10\n",
    "best_func = ''\n",
    "for name, f in add_funcs.items():\n",
    "    mse_train, mse_test = get_mse(f, X_train, y_train, X_test, y_test)\n",
    "    if mse_test < min_mse_test:\n",
    "        min_mse_test = mse_test\n",
    "        best_func = name\n",
    "    print('func:',name,'\\tmse_train =', mse_train, '\\tmse_test =', mse_test)\n",
    "print('\\nBest func:', best_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
