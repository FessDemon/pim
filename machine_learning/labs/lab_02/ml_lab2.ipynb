{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 2. Линейная регрессия. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним нормальное уравнение:\n",
    "\n",
    "$$\\overrightarrow{w}_{opt} = \\left(X^TX\\right)^{-1}X^T\\overrightarrow{y}.$$\n",
    "\n",
    "Здесь присутствует обращение матрицы $X^TX$ – довольно трудоёмкая операция при большом количестве признаков: сложность вычислений $O(d^3)$. При решении реальных задач такая трудоёмкость часто оказывается непозволительной, поэтому параметры модели (весовые коэффициенты) ищут итерационными методами, стоимость которых меньше. Один из них – *градиентный спуск* (gradient descent – ['greɪdɪənt dɪ'sent]).\n",
    "\n",
    "Напомним, что в градиентном спуске значения параметров на следующем шаге получаются из значений параметров на текущем шаге смещением в сторону антиградиента функционала ошибки: \n",
    "\n",
    "$$\\overrightarrow{w}^{(k+1)} = \\overrightarrow{w}^{(k)} - \\eta_k \\nabla Q(\\overrightarrow{w}^{(k)}),$$\n",
    "где $\\eta_k$ – шаг градиентного спуска.\n",
    "\n",
    "Формула градиента функционала ошибки выглядит следующим образом:\n",
    "\n",
    "$$\\nabla Q(\\overrightarrow{w}) = \\nabla_\\overrightarrow{w}\\left(\\frac{1}{l}\\|X\\overrightarrow{w}-\\overrightarrow{y}\\|^2\\right) = \\frac{2}{l}X^T(X\\overrightarrow{w} - \\overrightarrow{y}).$$\n",
    " \n",
    "Сложность вычислений в данном случае $O(dl)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1. Реализация градиентного спуска**  \n",
    "\n",
    "Напишите функцию `gradient_descent`, которая находит вектор весов на основе градиентного спуска.  \n",
    "\n",
    "В качестве критериев остановки можно использовать максимальное количество шагов и/или количество шагов, при котором отсутствуют значимые изменения весов.\n",
    "\n",
    "Проверьте работу функции на простом примере из лекций:\n",
    "\n",
    "$$x_1=2, x_2=3, x_3=5,$$\n",
    "\n",
    "$$y_1=1, y_2=3, y_3=4.$$\n",
    "\n",
    "Нарисуйте исходные данные и полученную линию регресии при помощи ``matplotlib``: для рисования точек используйте ``plt.scatter``, для рисования линии – ``plt.plot``.  \n",
    "\n",
    "Сравните полученные результаты с результатами, полученными на основе нормального уравнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2. Исследование скорости спуска**  \n",
    "\n",
    "Протестируйте функцию `gradient_descent` на наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` для разных значений скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$.  \n",
    "\n",
    "Оцените количество шагов для получения решения в каждом случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3. Стохастический градиентный спуск**  \n",
    "\n",
    "Стохастический градиентный спуск отличается от обычного заменой градиента на его оценку по одному или нескольким объектам. В этом случае сложность становится $O(kd)$, где $k$ – количество объектов, по которым оценивается градиент, $k<<l$. Это отчасти объясняет популярность стохастических методов оптимизации.  \n",
    "\n",
    "Реализуйте функцию `stochastic_gradient_descent`, которая находит вектор весов на основе стохастического градиентного спуска (вычисление градиента на одном случайном примере).  \n",
    "\n",
    "На наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` оцените количество шагов для получения решения при разных значениях скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4. Градиентный спуск по мини-батчам**  \n",
    "\n",
    "Реализуйте функцию `mini_batch_gradient_descent`, которая находит вектор весов на основе градиентного спуска по мини-батчам (вычисление градиента на подмножестве случайно выбранных примеров). Размер мини-батча должен быть параметром функции.  \n",
    "\n",
    "На наборе данных `ml_lab1_train.txt`/`ml_lab1_test.txt` оцените количество шагов для получения решения при разных значениях скорости спуска $\\eta_k$, например, $\\eta_k = \\{0.5, 1.0, 2.0\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
