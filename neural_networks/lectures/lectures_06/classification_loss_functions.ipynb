{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции потерь для задач классификации в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной функцией потерь для задач классификации является *перекрестная энтропия* (cross-entropy):  \n",
    "\n",
    "$$H(p,q)=-\\sum_{y}p(y)\\log q(y),$$\n",
    "где $p$ – истинное вероятностное распределение, $q$ – вероятностное распределение, предсказанное моделью, $y$ – класс.  \n",
    "\n",
    "В PyTorch существует несколько вариантов [функций потерь](https://pytorch.org/docs/stable/nn.html#loss-functions), связанных с перекрестной энтропией, основными из которых являются следующие:\n",
    "- [BCELoss](#Binary-Cross-Entropy-(BCELoss)) – перекрестная энтропия для бинарной классификации;\n",
    "- [CrossEntropyLoss](#Cross-Entropy-(CrossEntropyLoss)) – перекрестная энтропия для многоклассовой классификации;\n",
    "- [NLLLoss](#Negative-Log-Likelihood-(NLLLoss)) – отрицательное логарифмическое правдоподобие (negative log likelihood);\n",
    "- [BCEWithLogitsLoss](#Binary-Cross-Entropy-with-Logits-Loss-(BCEWithLogitsLoss)) – перекрестная энтропия для бинарной классификации с logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "precision = 8 # Точность после запятой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy (BCELoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Бинарная перекрестная энтропия* (Binary Cross-Entropy, [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)):  \n",
    "\n",
    "$$L = -\\frac{1}{l}\\sum_{i=1}^{l}y_i\\log{p(y_i)}+(1-y_i)\\log{(1-p(y_i))},$$\n",
    "\n",
    "где $y_i$ – истинный ответ, $y_i\\in\\{0,1\\}$;  \n",
    "$p(y_i)$ – предсказанная вероятность ответа $y_i=1$.  \n",
    "\n",
    "Применяется для задач бинарной классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy\n",
    "Обратите внимание на размерности массивов данных – это одномерные массивы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "y_pred = np.array([0.1, 0.9, 0.2])\n",
    "y_true = np.array([0.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy\n",
    "def BCE(y_pred, y_true):\n",
    "    bce_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss (NumPy) = 0.14462153\n"
     ]
    }
   ],
   "source": [
    "bce_loss_np = BCE(y_pred, y_true)\n",
    "print(f'BCE loss (NumPy) = {bce_loss_np:.{precision}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss (PyTorch) = 0.14462153\n"
     ]
    }
   ],
   "source": [
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_loss_torch = bce_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'BCE loss (PyTorch) = {bce_loss_torch:.{precision}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy (CrossEntropyLoss)\n",
    "Перекрестная энтропия в случае многоклассовой классификации (Categorical Cross-Entropy, Multiclass Cross-Entropy, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)):  \n",
    "\n",
    "$$L = -\\frac{1}{l}\\sum_{i=1}^{l}\\sum_{c \\in C}y_{ic}\\log{p(y_{ic})},$$\n",
    "\n",
    "где $l$ – количество примеров, $C$ – множество классов.\n",
    "\n",
    "В PyTorch в функции `CrossEntropyLoss()` предсказанные значения считаются ненормализованными, поэтому перед вычислением перекрестной энтропии к ним применяется функция`Softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy\n",
    "На входе двумерные массивы: количество строк совпадает с количеством примеров, количество столбцов – с количеством классов.  \n",
    "В каждой строке находятся вероятности принадлежности данного примера к классу, номер которого соответствует индексу столбца (от 0 до K–1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0,  1.0, 0.0],  [0.0,  0.0,  1.0], [1.0,  0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5017132 , 0.2491434 , 0.2491434 ],\n",
       "       [0.26175419, 0.50140083, 0.23684498],\n",
       "       [0.2304335 , 0.2304335 , 0.53913301],\n",
       "       [0.5203738 , 0.24580718, 0.23381902]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем Softmax, как в PyTorch\n",
    "y_pred_softmax = scipy.special.softmax(y_pred, axis=1)\n",
    "y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(y_pred, y_true):\n",
    "    total_loss = -np.sum(y_true * np.log(y_pred))\n",
    "    num_of_samples = y_pred.shape[0]\n",
    "    mean_loss = total_loss / num_of_samples\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy loss (NumPy) = 0.66276923\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_np = CrossEntropy(y_pred_softmax, y_true)\n",
    "print(f'Cross Entropy loss (NumPy) = {cross_entropy_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинарная классификация в NumPy (аналог BCELoss)\n",
    "Для примера покажем вычисление бинарной перекрестной энтропии при помощи функции `CrossEntropy()`.  \n",
    "Возьмем пример, который использовался выше в бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "# y_pred = np.array([0.1, 0.9, 0.2])\n",
    "# y_true = np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "# Преобразуем к двумерным массивам\n",
    "y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2]])\n",
    "y_true = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy loss (NumPy) = 0.14462153\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_np = CrossEntropy(y_pred, y_true)\n",
    "print(f'Cross Entropy loss (NumPy) = {cross_entropy_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch\n",
    "Вычисление многоклассовой перекрестной энтропии в PyTorch реализуется при помощи функции `CrossEntropyLoss()`.  \n",
    "На вход функции поступает двумерный массив предсказанных вероятностей (строки – примеры, столбцы – классы) и одномерный массив истинных классов: значение в этом массиве обозначает класс (от 0 до K–1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "\n",
    "# Преобразуем двумерный массив y_true в одномерный\n",
    "y_true = np.array([0, 1, 2, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy loss (PyTorch) = 0.66276923\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "cross_entropy_loss_torch = cross_entropy_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'Cross Entropy loss (PyTorch) = {cross_entropy_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинарная классификация в PyTorch (аналог BCELoss)\n",
    "Также для примера продемонстрируем вычисление бинарной перекрестной энтропии при помощи функции PyTorch `CrossEntropyLoss()`.\n",
    "Возьмем пример, который использовался выше в бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два класса: [0, 1]\n",
    "# Три примера\n",
    "# Числа обозначают вероятности отнесения примера к классу 1\n",
    "# y_pred = np.array([0.1, 0.9, 0.2])\n",
    "# y_true = np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "# Преобразуем к двумерным массивам\n",
    "y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2]])\n",
    "y_true = np.array([0, 1, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логарифмируем, чтобы компенсировать Softmax, встроенный в CrossEntropyLoss()\n",
    "y_pred_log = torch.log(y_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy loss (PyTorch) = 0.14462153\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()\n",
    "cross_entropy_loss_torch = cross_entropy_loss_fn(y_pred_log, y_true_tensor)\n",
    "print(f'Cross Entropy loss (PyTorch) = {cross_entropy_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood (NLLLoss)\n",
    "*Отрицательное логарифмическое правдоподобие* (Negative Log Likelihood, [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)) – это другое название перекрестной энтропии.  \n",
    "В PyTorch отрицательное логарифмическое правдоподобие реализуется при помощи функции `NLLLoss()`, но предполагается, что предсказанные ненормализованные значения $x_i$ преобразуются с помощью функции `LogSoftmax()`:\n",
    "\n",
    "$$LogSoftmax(x_i)=\\log\\left(\\frac{e^{x_i}}{\\sum_{j}x_j}\\right)$$\n",
    "\n",
    "Поэтому в PyTorch перед использованием `NLLLoss()` ненормализованные выходы следует преобразовать с использованием функции `LogSoftmax()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5017132 , 0.2491434 , 0.2491434 ],\n",
       "       [0.26175419, 0.50140083, 0.23684498],\n",
       "       [0.2304335 , 0.2304335 , 0.53913301],\n",
       "       [0.5203738 , 0.24580718, 0.23381902]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем Softmax\n",
    "y_pred_softmax = scipy.special.softmax(y_pred, axis=1)\n",
    "y_pred_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.68972664, -1.38972664, -1.38972664],\n",
       "       [-1.34034944, -0.69034944, -1.44034944],\n",
       "       [-1.46779297, -1.46779297, -0.61779297],\n",
       "       [-0.65320788, -1.40320788, -1.45320788]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Применяем логаримф - получается LogSoftmax\n",
    "y_pred_log_softmax = np.log(y_pred_softmax)\n",
    "y_pred_log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(y_pred, y_true):\n",
    "    # То же, что CrossEntropy, но без логарифма перед y_pred\n",
    "    total_loss = -np.sum(y_true * y_pred)\n",
    "    num_of_samples = y_pred.shape[0]\n",
    "    mean_loss = total_loss / num_of_samples\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood loss (NumPy) = 0.66276923\n"
     ]
    }
   ],
   "source": [
    "nll_loss_np = NLLLoss(y_pred_log_softmax, y_true)\n",
    "print(f'Negative Log Likelihood loss (NumPy) = {nll_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]])\n",
    "\n",
    "# Преобразуем двумерный массив y_true в одномерный\n",
    "y_true = np.array([0, 1, 2, 0]).astype(np.int64) # номера классов из диапазона [0, K-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "y_pred_tensor = softmax(y_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood loss (PyTorch) = 0.66276923\n"
     ]
    }
   ],
   "source": [
    "nll_loss_fn = nn.NLLLoss()\n",
    "nll_loss_torch = nll_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'Negative Log Likelihood loss (PyTorch) = {nll_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss)\n",
    "Функция [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) комбинирует `Sigmoid` и `BCELoss` в единый класс.  \n",
    "Применяется для многозначной (*multilabel*) классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Вычисление в NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера, для каждого из которых может быть несколько классов\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [1.0, 0.0, 1.0], [1.0, 1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCEWithLogitsLoss(y_pred, y_true, reduction='mean'):\n",
    "    y_pred_sigmoid = scipy.special.expit(y_pred)\n",
    "    \n",
    "    loss = y_true * np.log(y_pred_sigmoid) + (1 - y_true) * np.log(1 - y_pred_sigmoid)\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        bce_loss = -np.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        bce_loss = -np.sum(loss)\n",
    "    \n",
    "    # Или (поскольку среднее находится по всем элементам матриц, а не по количеству объектов, как в Cross-Entropy):\n",
    "    # sum_loss = -np.sum(loss)\n",
    "    # if reduction == 'mean':\n",
    "    #   bce_loss = sum_bce_loss / y_pred.size\n",
    "    \n",
    "    return bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy with Logits Loss (NumPy) = 0.58476716\n"
     ]
    }
   ],
   "source": [
    "bcewithlogits_loss_np = BCEWithLogitsLoss(y_pred, y_true, reduction='mean')\n",
    "print(f'Binary Cross-Entropy with Logits Loss (NumPy) = {bcewithlogits_loss_np:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать ранее определенную функцию для Binary Cross Entropy (предварительно применив сигмоиду):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE loss (NumPy) = 0.58476716\n"
     ]
    }
   ],
   "source": [
    "y_pred_sigmoid = scipy.special.expit(y_pred)\n",
    "bce_loss_np = BCE(y_pred_sigmoid, y_true)\n",
    "print(f'BCE loss (NumPy) = {bce_loss_np:.{precision}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вычисление в PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Три класса: [0, 1, 2]\n",
    "# Четыре примера, для каждого из которых может быть несколько классов\n",
    "y_pred = np.array([[0.8, 0.1, 0.1], [0.15, 0.8, 0.05], [0.05, 0.05, 0.9], [0.85, 0.1, 0.05]])\n",
    "y_true = np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [1.0, 0.0, 1.0], [1.0, 1.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = torch.from_numpy(y_pred)\n",
    "y_true_tensor = torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy with Logits Loss (PyTorch) = 0.58476716\n"
     ]
    }
   ],
   "source": [
    "bcewithlogits_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "bcewithlogits_loss_torch = bcewithlogits_loss_fn(y_pred_tensor, y_true_tensor)\n",
    "print(f'Binary Cross-Entropy with Logits Loss (PyTorch) = {bcewithlogits_loss_torch:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Резюме\n",
    "Обозначим `y` – ненормированный выход последнего слоя нейронной сети (например, выход модуля `Linear`).\n",
    "1. Бинарная классификация:\n",
    "    - `y` 🡒 `Sigmoid` 🡒 `BCELoss`\n",
    "2. Многоклассовая классификация:\n",
    "    - `y` 🡒 `LogSoftmax` 🡒 `NLLLoss`\n",
    "    - `y` 🡒 `CrossEntropyLoss` (аналог: `y` 🡒 `LogSoftmax` 🡒 `NLLLoss`)\n",
    "  \n",
    "Если требуется на выходе сети явно иметь результаты `Softmax`, можно использовать следующую схему:  \n",
    "- `y` 🡒 `Softmax` 🡒 `Log` 🡒 `NLLLoss`\n",
    "\n",
    "3. Многозначная (*multilabel*) классификация:\n",
    "    - `y` 🡒 `BCEWithLogitsLoss` (аналог: `y` 🡒 `Sigmoid` 🡒 `BCELoss`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
