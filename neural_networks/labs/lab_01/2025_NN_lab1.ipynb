{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1. Введение в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "1. [Установка](#Установка)\n",
    "1. [Тензоры](#Тензоры)\n",
    "1. [Автоматическое дифференцирование](#Автоматическое-дифференцирование)\n",
    "1. [Линейная регрессия в PyTorch](#Линейная-регрессия-в-PyTorch)\n",
    "1. [Линейная классификация в PyTorch](#Линейная-классификация-в-PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка\n",
    "Чтобы установить PyTorch, нужно на [официальном сайте](https://pytorch.org/get-started/locally/) сгенерировать команду для установки в зависимости от версии операционной системы и других параметров.  \n",
    "Рекомендуется также установить [CUDA](https://developer.nvidia.com/cuda-zone) (Compute Unified Device Architecture) – вычислительную платформу от Nvidia для поддержки вычислений на видеокартах и [CuDNN](https://developer.nvidia.com/cudnn) (CUDA Deep Neural Network) – библиотеку на основе CUDA для поддержки глубоких нейронных сетей.  \n",
    "Проверить правильность установки и узнать версии используемого ПО можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Утилита от Nvidia для вывода информации о видеокарте\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python VERSION:', sys.version)\n",
    "print('\\npyTorch VERSION:', torch.__version__)\n",
    "print('\\nCUDA VERSION:')\n",
    "! nvcc --version\n",
    "print('\\nCUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('\\nNumber CUDA Devices:', torch.cuda.device_count())\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Cuda device name: ', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Проверить доступность CUDA можно также следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тензоры\n",
    "*Тензор* – многомерный массив данных в библиотеках глубокого обучения. Тензор может быть 0-мерным (скаляр), одномерным (вектор), двумерным (матрица) или размерностью больше двух.  \n",
    "Тензоры используются для представления входов, выходов и параметров моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация тензоров\n",
    "Тензоры могут быть инициализированы несколькими способами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. На основе данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Из массивов NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Заполненные случайным образом или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'Random Tensor: \\n {rand_tensor} \\n')\n",
    "print(f'Ones Tensor: \\n {ones_tensor} \\n')\n",
    "print(f'Zeros Tensor: \\n {zeros_tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Атрибуты тензоров\n",
    "Тензоры имеют несколько атрибутов, в том числе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(f'Shape of tensor: {tensor.shape}')\n",
    "print(f'Datatype of tensor: {tensor.dtype}')\n",
    "print(f'Device tensor is stored on the following device: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Операции над тензорами\n",
    "По умолчанию тензоры создаются на CPU.  \n",
    "Чтобы переместить их на GPU нужно вызвать метод `to`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tensor is stored on: {tensor.device}')\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "print(f'Tensor is stored on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Индексирование и срезы (slicing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print(f'Initial tensor:\\n{tensor}\\n')\n",
    "print(f'First row: {tensor[0]}\\n')\n",
    "print(f'First column: {tensor[:, 0]}\\n')\n",
    "print(f'Last column: {tensor[..., -1]}\\n')\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Конкатенация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_tensor = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(concat_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Арифметические операции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 3)\n",
    "tensor1 = torch.ones(shape)\n",
    "tensor2 = torch.ones(shape)\n",
    "tensor_sum = tensor1 + tensor2\n",
    "print(f'tensor1 + tensor2 = \\n{tensor_sum}\\n')\n",
    "tensor_mul1 = tensor1 @ tensor2\n",
    "print(f'tensor1 @ tensor2 = \\n{tensor_mul1}\\n')\n",
    "tensor_mul2 = torch.matmul(tensor1, tensor2)\n",
    "print(f'torch.matmul(tensor1, tensor2) = \\n{tensor_mul2}\\n')\n",
    "tensor_mul3 = tensor1 * tensor2\n",
    "print(f'tensor1 * tensor2 = \\n{tensor_mul3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Тензор может состоять из одного элемента (скаляр). В этом случае для доступа к значению тензора можно воспользоваться методом `item`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa = tensor.sum()\n",
    "print(f'summa = {summa}, type(summa): {type(summa)}\\n')\n",
    "print(f'summa.item() = {summa.item()}, type(summa.item()): {type(summa.item())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Операции `in-place` – это операции, результат которых сохраняется в самом операнде. Обозначаются суфиксом `_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{tensor}\\n')\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматическое дифференцирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для настройки весов в нейронных сетях используется *алгоритм обратного распространения ошибки* (back propagation). В этом алгоритме веса изменяются в зависимости от градиента функции ошибки.  \n",
    "В PyTorch модуль `torch.autograd` отвечает за автоматическое дифференцирование на вычислительном графе.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сигмоидальную функцию и её производную:\n",
    "$$\\sigma(u)=\\frac{1}{1+e^{-u}}$$\n",
    "  \n",
    "$$\\sigma'(u)=\\frac{e^{-u}}{(1+e^{-u})^2}=\\frac{1}{(1+e^{-u})}\\frac{e^{-u}}{(1+e^{-u})}=\\frac{1}{(1+e^{-u})}\\frac{(1+e^{-u})-1}{(1+e^{-u})}=\\sigma(u)(1-\\sigma(u))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим эти функции в PyTorch и нарисуем их графики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return 1 / (1 + torch.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(u):\n",
    "    return sigmoid(u) * (1 - sigmoid(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = torch.tensor(np.linspace(-5, 5, 100))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, sigmoid_deriv(x), label='sigmoid_deriv')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим тензор, для которого мы хотели бы найти градиент (параметр `requires_grad=True`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.tensor(0.0, requires_grad=True)\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим тензор, которому присвоим значение функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(u)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В атрибуте `grad_fn` хранится ссылка на функцию, которая была использована для вычисления тензора.  \n",
    "В случае задания тензора пользователем – `grad_fn = None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'u.grad_fn: {u.grad_fn}\\n')\n",
    "print(f's.grad_fn: {s.grad_fn}\\n')\n",
    "a = u + u\n",
    "print(f'(u+u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u - u\n",
    "print(f'(u-u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u * u\n",
    "print(f'(u*u).grad_fn: {a.grad_fn}\\n')\n",
    "a = u / u\n",
    "print(f'(u/u).grad_fn: {a.grad_fn}\\n')\n",
    "a = 1 / u\n",
    "print(f'(1/u).grad_fn: {a.grad_fn}\\n')\n",
    "a = torch.mean(u)\n",
    "print(f'mean(u).grad_fn: {a.grad_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Для вычисления градиента необходимо вызвать метод `backward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение градиента после вызова `backward` хранится в атрибуте `grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что значение градиента (производной) совпадает со значением, вычисленным аналитически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_deriv(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что повторный вызов метода `backward` невозможен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раскомментируйте эту строку\n",
    "#s.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы повторно вызывать метод `backward`, следует указать при его вызове параметр `retain_graph=True`. Но имейте в виду, что в этом случае значение градиента будет накапливаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a * a\n",
    "b.backward(retain_graph=True)\n",
    "print(f'a.grad = {a.grad}')\n",
    "b.backward(retain_graph=True)\n",
    "print(f'a.grad = {a.grad}')\n",
    "b.backward()\n",
    "print(f'a.grad = {a.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости градиенты можно обнулить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.grad.data.zero_()\n",
    "u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Проверьте значения производной сигмоидальной функции, вычисленные в разных точках. Сравните их со значениями на графике функций.\n",
    "\n",
    "*Примечание:*\n",
    "Отметим, что в PyTorch, конечно, имеется [своя реализация](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) сигмоидальной функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2**. Посчитайте значения градиента при помощи сигмоидальной функции из PyTorch.\n",
    "\n",
    "*Замечание*. Мы можем найти градиенты только для листьев в графе вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приостановка отслеживания градиентов\n",
    "По умолчанию, PyTorch отслеживает историю вычислений и поддерживает вычисление градиентов для тензоров, у которых установлено `requires_grad=True`.  \n",
    "Однако иногда бывает необходимо приостановить отслеживание, например, когда сеть уже обучена и требуется выполнять только прямой проход или когда в процессе обучения сети нужно зафиксировать веса определенных слоев. В этом случае можно воспользоваться блоком `no_grad` или методом `detach()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigmoid(u)\n",
    "print(f's.requires_grad = {s.requires_grad}')\n",
    "\n",
    "s = s.detach()\n",
    "print(f's.requires_grad = {s.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание модели\n",
    "Построим однослойную сеть, состоящую из одного нейрона, которая аналогична обычной линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пространство имен `nn` в PyTorch предоставляет все необходимые блоки для построения нейронных сетей.  \n",
    "Каждый модуль наследует классу [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Нейронная сеть представляет собой также модуль, который содержит другие модули (слои)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим устройство, на котором будем обучать сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using \"{}\" device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем класс нашей нейронной сети (который должен являться наследником `nn.Module`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.regr = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.regr(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `__init__` является конструктором класса.  \n",
    "В первой строке конструктора вызывается конструктор базового класса `nn.Module`.  \n",
    "Во второй строке создается единственный слой, содержащий один нейрон, с помощью модуля [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).  \n",
    "Данный модуль осуществляет линейное преобразование входа $\\vec{x}$ в выход $\\vec{y}$ с учетом матрицы весов $W$:  \n",
    "\n",
    "$$\\vec{y}=\\vec{x}W^T+\\vec{b}.$$\n",
    "\n",
    "Первый аргумент для `Linear` – `in_features` – количество входов $x$, второй `out_features` – количество выходов $y$ (фактически, количество нейронов в данном слое). Существует также третий аргумент – `bias`, определяющий наличие вектора свободных коэффициентов $\\vec{b}$ (по умолчанию `bias=True`).  \n",
    "Матрица весов $W$ будет иметь размерность (`out_features`, `in_features`), вектор свободных коэффициентов $\\vec{b}$ – (`out_features`).  \n",
    "Значения $W$ и $\\vec{b}$ инициализируются случайным образом из равномерного распределения $U(-\\sqrt{k},\\sqrt{k})$, где $k=\\frac{1}{in\\_features}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что атрибут `requires_grad` для тензоров-весов сети автоматически устанавливается в `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `forward` определяет поток данных через нашу сеть (*прямой проход*). Аргумент `x` – это входные данные для сети. В нашем примере к `x` применяется линейное преобразование, результат которого является выходом сети (линейная регрессия)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Инициализируем генератор случайных чисел PyTorch для воспроизводимости результатов (для одинаковых начальных значений весов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем экземпляр класса `NeuralNetwork` и выводим структуру сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Перемещаем сеть на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Вывод значений весов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(model):\n",
    "    \n",
    "    for parameter in model.named_parameters():\n",
    "        print(f'{parameter[0]} = {parameter[1].data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем генератор случайных чисел `numpy` для воспроизводимости результатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем случайные данные:  \n",
    "- $X$ – из равномерного распределения от 1 до 10,\n",
    "- $y=2x+1$ с учетом нормально распределенного шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "data = np.random.uniform(1, 10, (n_samples, 1)).astype(np.float32)\n",
    "targets = 2 * data + 1 + np.random.normal(0, 2, (n_samples, 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data, targets)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем тензоры PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемещаем тензоры на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем предсказание модели со случайно инициализированными весами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(data)\n",
    "plt.scatter(data.cpu().numpy(), targets.cpu().numpy())\n",
    "plt.plot(data.cpu().detach().numpy(), predictions.cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение сети\n",
    "Сначала задаем функцию потерь ([MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем определим оптимизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим функцию, определяющую действия в процессе одной эпохи обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, targets, model, loss_fn, optimizer):\n",
    "    \n",
    "    # Устанавливаем для модели режим обучения: эквивалентно model.training = True\n",
    "    # Действует только на модули Dropout, BatchNorm, InstanceNorm (https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Вычисляем предсказание модели на одном объекте\n",
    "        pred = model(data[i])\n",
    "        \n",
    "        # Вычисляем функцию потерь\n",
    "        loss = loss_fn(pred, targets[i])\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем цикл обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "print_weights(model)\n",
    "for t in range(epochs):\n",
    "    print(f'-------------------------------\\nEpoch {t+1}')\n",
    "    train_loop(data, targets, model, loss_fn, optimizer)\n",
    "    print()\n",
    "    print_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Нарисуем получившуюся модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращаем модель в режим инференса:  эквивалентно model.training = False\n",
    "model.eval()\n",
    "\n",
    "predictions = model(data)\n",
    "plt.scatter(data.cpu().numpy(), targets.cpu().numpy())\n",
    "plt.plot(data.cpu().detach().numpy(), predictions.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1**. Вычислите MSE для итоговой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2**. Выполните перебор гиперпараметров – скорости обучения и количества эпох (не менее пяти значений на каждый гиперпараметр) и найдите комбинацию, при которой достигается минимум MSE на обучающих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная классификация в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация набора данных\n",
    "Сгенерируем набор данных для классификации: объекты с меткой \"0\" будут нормально распределены вокруг точки (-1, -1),  объекты с меткой \"1\" – вокруг точки (1, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "half = int(n_samples / 2)\n",
    "     \n",
    "x_class0 = np.random.normal(size=[half, 2]) + [-1, -1]\n",
    "y_class0 = np.zeros((half, 1))\n",
    "\n",
    "x_class1 = np.random.normal(size=[half, 2]) + [1, 1]\n",
    "y_class1 = np.ones((half, 1))\n",
    "\n",
    "data = np.vstack([x_class0, x_class1])\n",
    "targets = np.vstack([y_class0, y_class1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype(np.float32)\n",
    "targets = targets.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_class0[:, 0], x_class0[:, 1], c='red')\n",
    "plt.scatter(x_class1[:, 0], x_class1[:, 1], c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Постройте нейронную сеть – линейный классификатор, разбейте набор данных на обучающую и валидацинную подвыборки. Визуализируйте полученные подвыборки данных. Обучите модель распознавать приведенный набор данных.\n",
    "\n",
    "*Подсказка*: в качестве функции потерь можно использовать Binary Cross Entropy ([BCELOSS](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Исследуйте значение функции потерь на обучающей и валидационной подвыборках после 1, 5, 10, 50, 100, 200, 500, 1000 эпох. Визуализируйте результаты. Достигается ли переобучение после какой-либо из эпох?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Сгенерируйте новые наборы данных при `n_samples` равном 100, 500, 1000. Проанализируйте качество обучения при разном объёме сгнерированных данных. Постройте график `loss` (на валидационной выборке) -`n_samples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4 (факультативное)\n",
    "Прочитать параграф 2.6 в учебнике С. Николенко и др. (стр. 81–92) и воспроизвести рассмотренные там нейронные сети на TensorFlow и Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
