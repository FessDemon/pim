{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150326cf",
   "metadata": {},
   "source": [
    "## Лабораторная работа 1. Морфологический парсер mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f96232",
   "metadata": {},
   "source": [
    "**Задание 1.** Изучите документацию и лицензию (!) морфологического парсера mystem от Yandex: https://yandex.ru/dev/mystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc662",
   "metadata": {},
   "source": [
    "**Задание 2.** Установите `pymystem3` – интерфейс к mystem на Python: https://pypi.org/project/pymystem3.\n",
    "\n",
    "(!) Обратите внимание, что у конструктора объекта Mystem() есть параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc49582",
   "metadata": {},
   "source": [
    "**Задание 3.** Выпишите с какими параметрами запускается морфологический анализатор.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899b2a2",
   "metadata": {},
   "source": [
    "-n - Построчный режим; каждое слово печатается на новой строке.\n",
    "\n",
    "-c - Копировать весь ввод на вывод. То есть, не только слова, но и межсловные промежутки.\n",
    "\n",
    "-w - Печатать только словарные слова.\n",
    "\n",
    "-l - Не печатать исходные словоформы, только леммы и граммемы.\n",
    "\n",
    "-i - Печатать грамматическую информацию, расшифровка ниже.\n",
    "\n",
    "-g - Склеивать информацию словоформ при одной лемме (только при включенной опции -i).\n",
    "\n",
    "-s - Печатать маркер конца предложения (только при включенной опции -c).\n",
    "\n",
    "-e - Кодировка ввода/вывода. Возможные варианты: cp866, cp1251, koi8-r, utf-8 (по умолчанию).\n",
    "\n",
    "-d - Применить контекстное снятие омонимии.\n",
    "\n",
    "--eng-gr - Печатать английские обозначения граммем.\n",
    "\n",
    "--filter-gram - Строить разборы только с указанными граммемами.\n",
    "\n",
    "--fixlist - Использовать файл с пользовательским словарём.\n",
    "\n",
    "--format - Формат вывода. Возможные варианты: text, xml, json. Значение по умолчанию — text.\n",
    "\n",
    "--generate-all - Генерировать все возможные гипотезы для несловарных слов.\n",
    "\n",
    "--weight - Печатать бесконтекстную вероятность леммы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d20a7",
   "metadata": {},
   "source": [
    "**а)** Придумайте и запишите примеры предложений со словами не из словаря. Приведите их полученные морфологические разборы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c077617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Предложение 1: Мой друг стал инфлюенсером в социальных сетях.\n",
      "Морфологический анализ:\n",
      "  Мой -> лемма: мой, грамм. информация: APRO=(вин,ед,муж,неод|им,ед,муж)\n",
      "  друг -> лемма: друг, грамм. информация: S,муж,од=им,ед\n",
      "  стал -> лемма: становиться, грамм. информация: V,нп=прош,ед,изъяв,муж,сов\n",
      "  инфлюенсером -> лемма: инфлюенсер, грамм. информация: S,гео,муж,неод=твор,ед\n",
      "  в -> лемма: в, грамм. информация: PR=\n",
      "  социальных -> лемма: социальный, грамм. информация: A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)\n",
      "  сетях -> лемма: сеть, грамм. информация: S,жен,неод=пр,мн\n",
      "  . -> Слово отсутствует в словаре\n",
      "\n",
      "Предложение 2: Этот стартап получил значительный фандрайзинг.\n",
      "Морфологический анализ:\n",
      "  Этот -> лемма: этот, грамм. информация: APRO=(вин,ед,муж,неод|им,ед,муж)\n",
      "  стартап -> лемма: стартап, грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "  получил -> лемма: получать, грамм. информация: V,пе=прош,ед,изъяв,муж,сов\n",
      "  значительный -> лемма: значительный, грамм. информация: A=(вин,ед,полн,муж,неод|им,ед,полн,муж)\n",
      "  фандрайзинг -> лемма: фандрайзинг, грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "  . -> Слово отсутствует в словаре\n",
      "\n",
      "Предложение 3: Смартфон имеет отличную автономность и функциональность.\n",
      "Морфологический анализ:\n",
      "  Смартфон -> лемма: смартфон, грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "  имеет -> лемма: иметь, грамм. информация: V,несов,пе=непрош,ед,изъяв,3-л\n",
      "  отличную -> лемма: отличный, грамм. информация: A=вин,ед,полн,жен\n",
      "  автономность -> лемма: автономность, грамм. информация: S,жен,неод=(вин,ед|им,ед)\n",
      "  и -> лемма: и, грамм. информация: CONJ=\n",
      "  функциональность -> лемма: функциональность, грамм. информация: S,жен,неод=(вин,ед|им,ед)\n",
      "  . -> Слово отсутствует в словаре\n",
      "\n",
      "Предложение 4: Блогер создал вирусный контент для своей аудитории.\n",
      "Морфологический анализ:\n",
      "  Блогер -> лемма: блогер, грамм. информация: S,имя,муж,од=им,ед\n",
      "  создал -> лемма: создавать, грамм. информация: V,пе=прош,ед,изъяв,муж,сов\n",
      "  вирусный -> лемма: вирусный, грамм. информация: A=(вин,ед,полн,муж,неод|им,ед,полн,муж)\n",
      "  контент -> лемма: контент, грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "  для -> лемма: для, грамм. информация: PR=\n",
      "  своей -> лемма: свой, грамм. информация: APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен)\n",
      "  аудитории -> лемма: аудитория, грамм. информация: S,жен,неод=(пр,ед|вин,мн|дат,ед|род,ед|им,мн)\n",
      "  . -> Слово отсутствует в словаре\n",
      "\n",
      "Предложение 5: Компания проводит ребрендинг своего бренда.\n",
      "Морфологический анализ:\n",
      "  Компания -> лемма: компания, грамм. информация: S,жен,неод=им,ед\n",
      "  проводит -> лемма: проводить, грамм. информация: V=непрош,ед,изъяв,3-л,несов,пе\n",
      "  ребрендинг -> лемма: ребрендинг, грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "  своего -> лемма: свой, грамм. информация: APRO=(вин,ед,муж,од|род,ед,муж|род,ед,сред)\n",
      "  бренда -> лемма: бренд, грамм. информация: S,муж,неод=род,ед\n",
      "  . -> Слово отсутствует в словаре\n",
      "\n",
      "Предложение 6: Калушата дудонили бутявку.\n",
      "Морфологический анализ:\n",
      "  Калушата -> лемма: калушенок, грамм. информация: S,муж,од=им,мн\n",
      "  дудонили -> лемма: дудонить, грамм. информация: V,несов,пе=прош,мн,изъяв\n",
      "  бутявку -> лемма: бутявка, грамм. информация: S,жен,неод=вин,ед\n",
      "  . -> Слово отсутствует в словаре\n"
     ]
    }
   ],
   "source": [
    "m = Mystem()\n",
    "\n",
    "# Примеры предложений с потенциально несловарными словами\n",
    "sentences = [\n",
    "    \"Мой друг стал инфлюенсером в социальных сетях.\",\n",
    "    \"Этот стартап получил значительный фандрайзинг.\",\n",
    "    \"Смартфон имеет отличную автономность и функциональность.\",\n",
    "    \"Блогер создал вирусный контент для своей аудитории.\",\n",
    "    \"Компания проводит ребрендинг своего бренда.\",\n",
    "    \"Калушата дудонили бутявку.\"\n",
    "]\n",
    "\n",
    "# Проведем анализ для каждого предложения\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"\\nПредложение {i+1}: {sentence}\")\n",
    "    analyses = m.analyze(sentence)\n",
    "\n",
    "    print(\"Морфологический анализ:\")\n",
    "    for analysis in analyses:\n",
    "        text = analysis.get('text', '')\n",
    "        # Пропускаем пустые и пробельные элементы\n",
    "        if text.strip() and analysis.get('analysis'):\n",
    "            # Если слово есть в словаре\n",
    "            if analysis['analysis']:\n",
    "                lex = analysis['analysis'][0].get('lex', 'Неизвестно')\n",
    "                gr = analysis['analysis'][0].get('gr', 'Неизвестно')\n",
    "                print(f\"  {text} -> лемма: {lex}, грамм. информация: {gr}\")\n",
    "            else:\n",
    "                # Если слово отсутствует в словаре\n",
    "                print(f\"  {text} -> Слово отсутствует в словаре\")\n",
    "        elif text.strip():\n",
    "            # Если слово отсутствует в словаре (нет анализа)\n",
    "            print(f\"  {text} -> Слово отсутствует в словаре\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da94b8",
   "metadata": {},
   "source": [
    "**б)** Применяется ли контекстное снятие омонимии при морфологическом разборе?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5595923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение: Он потерял ключ от квартиры.\n",
      "  Лемма: ключ\n",
      "  Грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "\n",
      "Предложение: Источник ключ бьет в скале.\n",
      "  Лемма: ключ\n",
      "  Грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "\n",
      "Предложение: Нашел ключ к решению проблемы.\n",
      "  Лемма: ключ\n",
      "  Грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "\n",
      "Предложение: Слесарь взял ключ на 13 мм.\n",
      "  Лемма: ключ\n",
      "  Грамм. информация: S,муж,неод=(вин,ед|им,ед)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Примеры с разным контекстом для слова \"ключ\"\n",
    "sentences = [\n",
    "    \"Он потерял ключ от квартиры.\",  # Существительное, мужской род\n",
    "    \"Источник ключ бьет в скале.\",   # Существительное, мужской род\n",
    "    \"Нашел ключ к решению проблемы.\",      # Существительное, мужской род\n",
    "    \"Слесарь взял ключ на 13 мм.\",        # Существительное, мужской род\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    analyses = m.analyze(sentence)\n",
    "    print(f\"Предложение: {sentence}\")\n",
    "    for analysis in analyses:\n",
    "        if analysis['text'].strip() == 'ключ' and 'analysis' in analysis:\n",
    "            if analysis['analysis']:\n",
    "                print(f\"  Лемма: {analysis['analysis'][0]['lex']}\")\n",
    "                print(f\"  Грамм. информация: {analysis['analysis'][0]['gr']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18ab99",
   "metadata": {},
   "source": [
    "**Задание 4.** Напишите функцию `parse_text()`, на вход которой поступает текст (в виде строки), а на выходе формируется структура данных, содержащая для каждого слова входного текста следующую информацию:\n",
    "- исходную словоформу (wordform);\n",
    "- нормальную форму слова (лемму) (norm, lemma);\n",
    "- часть речи (part of speech, POS);\n",
    "- другую грамматическую информацию, выдаваемую mystem;\n",
    "- признак, присутствует ли слово в словаре mystem.\n",
    "\n",
    "Функция должна выбирать наиболее вероятный вариант морфологического разбора слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31808721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словоформа      Лемма           Часть речи   В словаре  Грамм. инфо\n",
      "--------------------------------------------------------------------------------\n",
      "Прекрасный      прекрасный      A=(вин       Да         A=(вин,ед,полн,муж,неод|им,ед,полн,муж)\n",
      "день            день            S            Да         S,муж,неод=(вин,ед|им,ед)\n",
      "Солнце          солнце          S            Да         S,сред,неод=(пр,ед|вин,ед|им,ед)\n",
      "светит          светить         V            Да         V,несов,нп=непрош,ед,изъяв,3-л\n",
      "ярко            ярко            ADV=         Да         ADV=\n",
      "птицы           птица           S            Да         S,жен,од=(род,ед|им,мн)\n",
      "поют            петь            V            Да         V,несов,пе=непрош,мн,изъяв,3-л\n",
      "Как             как             ADVPRO=      Да         ADVPRO=\n",
      "хорошо          хорошо          ADV=вводн    Да         ADV=вводн\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_text(text):\n",
    "\n",
    "    # Создаем экземпляр анализатора MyStem\n",
    "    m = Mystem()\n",
    "\n",
    "    # Проводим морфологический анализ текста\n",
    "    analyses = m.analyze(text)\n",
    "\n",
    "    # Список для хранения результатов\n",
    "    result = []\n",
    "\n",
    "    # Обрабатываем каждый элемент анализа\n",
    "    for analysis in analyses:\n",
    "        # Получаем исходную словоформу\n",
    "        wordform = analysis.get('text', '')\n",
    "\n",
    "        # Пропускаем пустые элементы и пробелы\n",
    "        if not wordform.strip():\n",
    "            continue\n",
    "\n",
    "        # Создаем словарь для хранения информации о слове\n",
    "        word_info = {\n",
    "            'wordform': wordform,\n",
    "            'lemma': None,\n",
    "            'pos': None,\n",
    "            'gramm_info': None,\n",
    "            'in_dictionary': False\n",
    "        }\n",
    "\n",
    "        # Проверяем наличие анализа\n",
    "        if 'analysis' in analysis and analysis['analysis']:\n",
    "            # Слово есть в словаре\n",
    "            word_info['in_dictionary'] = True\n",
    "\n",
    "            # Берем первый (наиболее вероятный) вариант анализа\n",
    "            # MyStem сам выбирает наиболее вероятный вариант на основе контекста\n",
    "            first_analysis = analysis['analysis'][0]\n",
    "\n",
    "            # Получаем лемму\n",
    "            word_info['lemma'] = first_analysis.get('lex', wordform)\n",
    "\n",
    "            # Получаем грамматическую информацию\n",
    "            gramm_info = first_analysis.get('gr', '')\n",
    "            word_info['gramm_info'] = gramm_info\n",
    "\n",
    "            # Извлекаем часть речи (первый элемент до запятой)\n",
    "            if gramm_info:\n",
    "                pos = gramm_info.split(',')[0]\n",
    "                word_info['pos'] = pos\n",
    "        else:\n",
    "            # Слово отсутствует в словаре\n",
    "            word_info['in_dictionary'] = False\n",
    "            # Для несловарных слов используем исходную форму как лемму\n",
    "            word_info['lemma'] = wordform\n",
    "            word_info['pos'] = 'UNKNOWN'\n",
    "            word_info['gramm_info'] = 'WORD_NOT_IN_DICTIONARY'\n",
    "\n",
    "        # Добавляем информацию о слове в результат\n",
    "        result.append(word_info)\n",
    "\n",
    "    return result\n",
    "\n",
    "def print_parsed_text(parsed_words):\n",
    "\n",
    "    print(f\"{'Словоформа':<15} {'Лемма':<15} {'Часть речи':<12} {'В словаре':<10} {'Грамм. инфо'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for word_info in parsed_words:\n",
    "        # Получаем словоформу\n",
    "        wordform = word_info['wordform']\n",
    "\n",
    "        # Проверяем, содержит ли элемент хотя бы одну букву или цифру\n",
    "        if re.search(r'[а-яА-ЯёЁa-zA-Z0-9]', wordform):\n",
    "            # Дополнительно проверяем, что элемент не состоит только из знаков препинания\n",
    "            if not re.fullmatch(r'[^\\w\\s]', wordform):\n",
    "                print(f\"{word_info['wordform']:<15} {word_info['lemma']:<15} {word_info['pos']:<12} \"\n",
    "                      f\"{'Да' if word_info['in_dictionary'] else 'Нет':<10} {word_info['gramm_info']}\")\n",
    "\n",
    "# Текст с различными знаками препинания\n",
    "text = \"Прекрасный день! Солнце светит ярко, птицы поют... Как хорошо!\"\n",
    "\n",
    "# Анализ текста\n",
    "parsed = parse_text(text)\n",
    "\n",
    "# Вывод результатов (без знаков препинания)\n",
    "print_parsed_text(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45237dc3",
   "metadata": {},
   "source": [
    "**Задание 5.** Напишите функцию `save_morph_results()`, сохраняющую структуру данных, получаемую функцией `parse_text()`, в текстовый файл формата JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ddbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты успешно сохранены в файл: morph_analysis.json\n",
      "Результаты успешно загружены из файла: morph_analysis.json\n",
      "Словоформа      Лемма           Часть речи   В словаре  Грамм. инфо\n",
      "--------------------------------------------------------------------------------\n",
      "Мама            мама            S            Да         S,жен,од=им,ед\n",
      "мыла            мыть            V            Да         V,несов,пе=прош,ед,изъяв,жен\n",
      "раму            рама            S            Да         S,жен,неод=вин,ед\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_text(text):\n",
    "    # Создаем экземпляр анализатора MyStem\n",
    "    m = Mystem()\n",
    "\n",
    "    # Проводим морфологический анализ текста\n",
    "    analyses = m.analyze(text)\n",
    "\n",
    "    # Список для хранения результатов\n",
    "    result = []\n",
    "\n",
    "    # Обрабатываем каждый элемент анализа\n",
    "    for analysis in analyses:\n",
    "        # Получаем исходную словоформу\n",
    "        wordform = analysis.get('text', '')\n",
    "\n",
    "        # Пропускаем пустые элементы и пробелы\n",
    "        if not wordform.strip():\n",
    "            continue\n",
    "\n",
    "        # Создаем словарь для хранения информации о слове\n",
    "        word_info = {\n",
    "            'wordform': wordform,\n",
    "            'lemma': None,\n",
    "            'pos': None,\n",
    "            'gramm_info': None,\n",
    "            'in_dictionary': False\n",
    "        }\n",
    "\n",
    "        # Проверяем наличие анализа\n",
    "        if 'analysis' in analysis and analysis['analysis']:\n",
    "            # Слово есть в словаре\n",
    "            word_info['in_dictionary'] = True\n",
    "\n",
    "            # Берем первый (наиболее вероятный) вариант анализа\n",
    "            # MyStem сам выбирает наиболее вероятный вариант на основе контекста\n",
    "            first_analysis = analysis['analysis'][0]\n",
    "\n",
    "            # Получаем лемму\n",
    "            word_info['lemma'] = first_analysis.get('lex', wordform)\n",
    "\n",
    "            # Получаем грамматическую информацию\n",
    "            gramm_info = first_analysis.get('gr', '')\n",
    "            word_info['gramm_info'] = gramm_info\n",
    "\n",
    "            # Извлекаем часть речи (первый элемент до запятой)\n",
    "            if gramm_info:\n",
    "                pos = gramm_info.split(',')[0]\n",
    "                word_info['pos'] = pos\n",
    "        else:\n",
    "            # Слово отсутствует в словаре\n",
    "            word_info['in_dictionary'] = False\n",
    "            # Для несловарных слов используем исходную форму как лемму\n",
    "            word_info['lemma'] = wordform\n",
    "            word_info['pos'] = 'UNKNOWN'\n",
    "            word_info['gramm_info'] = 'WORD_NOT_IN_DICTIONARY'\n",
    "\n",
    "        # Добавляем информацию о слове в результат\n",
    "        result.append(word_info)\n",
    "\n",
    "    return result\n",
    "\n",
    "def save_morph_results(parsed_data, filename):\n",
    "\n",
    "    # Проверяем, что parsed_data является списком\n",
    "    if not isinstance(parsed_data, list):\n",
    "        raise TypeError(\"Ожидается список данных для сохранения\")\n",
    "\n",
    "    # Создаем директорию для файла, если она не существует\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Открываем файл для записи в формате JSON\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Сохраняем данные в формате JSON с отступами для читаемости\n",
    "        json.dump(parsed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Результаты успешно сохранены в файл: {filename}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_morph_results(filename):\n",
    "    try:\n",
    "        # Проверяем существование файла\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"Файл {filename} не найден\")\n",
    "\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            parsed_data = json.load(f)\n",
    "\n",
    "        # Проверяем, что загруженные данные являются списком\n",
    "        if not isinstance(parsed_data, list):\n",
    "            raise TypeError(\"Загруженные данные не являются списком\")\n",
    "\n",
    "        print(f\"Результаты успешно загружены из файла: {filename}\")\n",
    "        return parsed_data\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Ошибка: Файл {filename} содержит некорректные данные JSON\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"Ошибка доступа: Нет прав для чтения файла {filename}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Неожиданная ошибка при загрузке из файла: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_parsed_text(parsed_words):\n",
    "\n",
    "    print(f\"{'Словоформа':<15} {'Лемма':<15} {'Часть речи':<12} {'В словаре':<10} {'Грамм. инфо'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for word_info in parsed_words:\n",
    "        # Получаем словоформу\n",
    "        wordform = word_info['wordform']\n",
    "\n",
    "        # Проверяем, содержит ли элемент хотя бы одну букву или цифру\n",
    "        if re.search(r'[а-яА-ЯёЁa-zA-Z0-9]', wordform):\n",
    "            # Дополнительно проверяем, что элемент не состоит только из знаков препинания\n",
    "            if not re.fullmatch(r'[^\\w\\s]', wordform):\n",
    "                print(f\"{word_info['wordform']:<15} {word_info['lemma']:<15} {word_info['pos']:<12} \"\n",
    "                      f\"{'Да' if word_info['in_dictionary'] else 'Нет':<10} {word_info['gramm_info']}\")\n",
    "\n",
    "\n",
    "# Анализ текста\n",
    "text = \"Мама мыла раму.\"\n",
    "parsed_data = parse_text(text)\n",
    "\n",
    "# Сохранение результатов в JSON файл\n",
    "success = save_morph_results(parsed_data, \"morph_analysis.json\")\n",
    "\n",
    "# Загрузка результатов из JSON файла\n",
    "if success:\n",
    "    loaded_data = load_morph_results(\"morph_analysis.json\")\n",
    "    if loaded_data:\n",
    "        print_parsed_text(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621548a6",
   "metadata": {},
   "source": [
    "**Задание 6.** Напишите функцию `get_dictionary()`, на вход которой поступает текст (в виде строки), а на выходе формируется словарь,\n",
    "включающий все уникальные слова текста и содержащий для каждого слова следующую информацию:\n",
    "- нормальную форму слова;\n",
    "- часть речи;\n",
    "- частоту слова в тексте;\n",
    "- все варианты словоформ в тексте с данной нормальной формой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf5fdae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нормальная форма Часть речи   Частота  Словоформы\n",
      "----------------------------------------------------------------------\n",
      "мама            S            2        Мамы, Мама\n",
      "мыть            V            2        мыла, моют\n",
      "рама            S            2        раму, рамы\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    # Создаем экземпляр анализатора MyStem\n",
    "    m = Mystem()\n",
    "\n",
    "    # Проводим морфологический анализ текста\n",
    "    analyses = m.analyze(text)\n",
    "\n",
    "    # Список для хранения результатов\n",
    "    result = []\n",
    "\n",
    "    # Обрабатываем каждый элемент анализа\n",
    "    for analysis in analyses:\n",
    "        # Получаем исходную словоформу\n",
    "        wordform = analysis.get('text', '')\n",
    "\n",
    "        # Пропускаем пустые элементы и пробелы\n",
    "        if not wordform.strip():\n",
    "            continue\n",
    "\n",
    "        # Создаем словарь для хранения информации о слове\n",
    "        word_info = {\n",
    "            'wordform': wordform,\n",
    "            'lemma': None,\n",
    "            'pos': None,\n",
    "            'gramm_info': None,\n",
    "            'in_dictionary': False\n",
    "        }\n",
    "\n",
    "        # Проверяем наличие анализа\n",
    "        if 'analysis' in analysis and analysis['analysis']:\n",
    "            # Слово есть в словаре\n",
    "            word_info['in_dictionary'] = True\n",
    "\n",
    "            # Берем первый (наиболее вероятный) вариант анализа\n",
    "            # MyStem сам выбирает наиболее вероятный вариант на основе контекста\n",
    "            first_analysis = analysis['analysis'][0]\n",
    "\n",
    "            # Получаем лемму\n",
    "            word_info['lemma'] = first_analysis.get('lex', wordform)\n",
    "\n",
    "            # Получаем грамматическую информацию\n",
    "            gramm_info = first_analysis.get('gr', '')\n",
    "            word_info['gramm_info'] = gramm_info\n",
    "\n",
    "            # Извлекаем часть речи (первый элемент до запятой)\n",
    "            if gramm_info:\n",
    "                pos = gramm_info.split(',')[0]\n",
    "                word_info['pos'] = pos\n",
    "        else:\n",
    "            # Слово отсутствует в словаре\n",
    "            word_info['in_dictionary'] = False\n",
    "            # Для несловарных слов используем исходную форму как лемму\n",
    "            word_info['lemma'] = wordform\n",
    "            word_info['pos'] = 'UNKNOWN'\n",
    "            word_info['gramm_info'] = 'WORD_NOT_IN_DICTIONARY'\n",
    "\n",
    "        # Добавляем информацию о слове в результат\n",
    "        result.append(word_info)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_dictionary(text):\n",
    "    # Получаем результаты морфологического анализа\n",
    "    parsed_words = parse_text(text)\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    dictionary = {}\n",
    "\n",
    "    # Словарь для подсчета частоты слов (по леммам)\n",
    "    frequency_counter = defaultdict(int)\n",
    "\n",
    "    # Словарь для хранения всех словоформ для каждой леммы\n",
    "    wordforms_dict = defaultdict(set)\n",
    "\n",
    "    # Обрабатываем каждое слово из результатов анализа\n",
    "    for word_info in parsed_words:\n",
    "        # Получаем словоформу\n",
    "        wordform = word_info['wordform']\n",
    "\n",
    "        # Проверяем, содержит ли элемент хотя бы одну букву или цифру\n",
    "        if not re.search(r'[а-яА-ЯёЁa-zA-Z0-9]', wordform):\n",
    "            continue\n",
    "\n",
    "        # Дополнительно проверяем, что элемент не состоит только из знаков препинания\n",
    "        if re.fullmatch(r'[^\\w\\s]', wordform):\n",
    "            continue\n",
    "\n",
    "        # Получаем лемму (нормальную форму)\n",
    "        lemma = word_info['lemma']\n",
    "\n",
    "        # Если лемма не определена, используем словоформу\n",
    "        if not lemma:\n",
    "            lemma = wordform\n",
    "\n",
    "        # Увеличиваем счетчик частоты для данной леммы\n",
    "        frequency_counter[lemma] += 1\n",
    "\n",
    "        # Добавляем словоформу в множество словоформ для данной леммы\n",
    "        wordforms_dict[lemma].add(wordform)\n",
    "\n",
    "        # Если лемма еще не добавлена в словарь, создаем для нее запись\n",
    "        if lemma not in dictionary:\n",
    "            dictionary[lemma] = {\n",
    "                'norm': lemma,                    # Нормальная форма слова\n",
    "                'pos': word_info['pos'],          # Часть речи\n",
    "                'frequency': 0,                   # Частота (будет обновлена позже)\n",
    "                'wordforms': []                   # Варианты словоформ (будет обновлен позже)\n",
    "            }\n",
    "        else:\n",
    "            # Если для этой леммы уже есть запись, но часть речи еще не определена,\n",
    "            # пытаемся уточнить ее из текущего слова\n",
    "            if dictionary[lemma]['pos'] in ['UNKNOWN', None] and word_info['pos'] not in ['UNKNOWN', None]:\n",
    "                dictionary[lemma]['pos'] = word_info['pos']\n",
    "\n",
    "    # Обновляем частоту и словоформы для каждой леммы\n",
    "    for lemma in dictionary:\n",
    "        dictionary[lemma]['frequency'] = frequency_counter[lemma]\n",
    "        dictionary[lemma]['wordforms'] = list(wordforms_dict[lemma])\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def print_dictionary(dictionary):\n",
    "\n",
    "    print(f\"{'Нормальная форма':<15} {'Часть речи':<12} {'Частота':<8} {'Словоформы'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for lemma, info in dictionary.items():\n",
    "        wordforms_str = ', '.join(info['wordforms'])\n",
    "        print(f\"{info['norm']:<15} {info['pos']:<12} {info['frequency']:<8} {wordforms_str}\")\n",
    "\n",
    "\n",
    "# Анализ текста\n",
    "text = \"Мама мыла раму. Мамы моют рамы.\"\n",
    "dictionary = get_dictionary(text)\n",
    "\n",
    "# Вывод результатов\n",
    "print_dictionary(dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb0797",
   "metadata": {},
   "source": [
    "**Задание 7.** Напишите функцию `save_dictionary()`, сохраняющую предыдущую структуру данных в текстовый файл формата JSON. Слова в файле должны быть упорядочены по убыванию частоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10a36600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словарь успешно сохранен в файл: word_dictionary.json\n"
     ]
    }
   ],
   "source": [
    "def save_dictionary(dictionary, filename):\n",
    "    \"\"\"\n",
    "    Функция для сохранения словаря в файл формата JSON.\n",
    "\n",
    "    Аргументы:\n",
    "        dictionary (dict): Словарь слов\n",
    "        filename (str): Имя файла для сохранения\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictionary, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Словарь успешно сохранен в файл: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении словаря: {e}\")\n",
    "\n",
    "# Сохранение словаря в файл\n",
    "save_dictionary(dictionary, \"word_dictionary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bdd0c",
   "metadata": {},
   "source": [
    "**Задание 8.** Напишите функцию `get_non_mystem_dict()`, на вход которой поступает структура данных, получаемая функцией `parse_text()`, а на выходе формируется словарь, содержащий уникальные слова текста, отсутствующие в словаре mystem, вместе с частотой слова в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7299673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова, отсутствующие в словаре mystem, не найдены.\n"
     ]
    }
   ],
   "source": [
    "def parse_text(text):\n",
    "    # Создаем экземпляр анализатора MyStem\n",
    "    m = Mystem()\n",
    "\n",
    "    # Проводим морфологический анализ текста\n",
    "    analyses = m.analyze(text)\n",
    "\n",
    "    # Список для хранения результатов\n",
    "    result = []\n",
    "\n",
    "    # Обрабатываем каждый элемент анализа\n",
    "    for analysis in analyses:\n",
    "        # Получаем исходную словоформу\n",
    "        wordform = analysis.get('text', '')\n",
    "\n",
    "        # Пропускаем пустые элементы и пробелы\n",
    "        if not wordform.strip():\n",
    "            continue\n",
    "\n",
    "        # Создаем словарь для хранения информации о слове\n",
    "        word_info = {\n",
    "            'wordform': wordform,\n",
    "            'lemma': None,\n",
    "            'pos': None,\n",
    "            'gramm_info': None,\n",
    "            'in_dictionary': False\n",
    "        }\n",
    "\n",
    "        # Проверяем наличие анализа\n",
    "        if 'analysis' in analysis and analysis['analysis']:\n",
    "            # Слово есть в словаре\n",
    "            word_info['in_dictionary'] = True\n",
    "\n",
    "            # Берем первый (наиболее вероятный) вариант анализа\n",
    "            # MyStem сам выбирает наиболее вероятный вариант на основе контекста\n",
    "            first_analysis = analysis['analysis'][0]\n",
    "\n",
    "            # Получаем лемму\n",
    "            word_info['lemma'] = first_analysis.get('lex', wordform)\n",
    "\n",
    "            # Получаем грамматическую информацию\n",
    "            gramm_info = first_analysis.get('gr', '')\n",
    "            word_info['gramm_info'] = gramm_info\n",
    "\n",
    "            # Извлекаем часть речи (первый элемент до запятой)\n",
    "            if gramm_info:\n",
    "                pos = gramm_info.split(',')[0]\n",
    "                word_info['pos'] = pos\n",
    "        else:\n",
    "            # Слово отсутствует в словаре\n",
    "            word_info['in_dictionary'] = False\n",
    "            # Для несловарных слов используем исходную форму как лемму\n",
    "            word_info['lemma'] = wordform\n",
    "            word_info['pos'] = 'UNKNOWN'\n",
    "            word_info['gramm_info'] = 'WORD_NOT_IN_DICTIONARY'\n",
    "\n",
    "        # Добавляем информацию о слове в результат\n",
    "        result.append(word_info)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_non_mystem_dict(parsed_data):\n",
    "    # Словарь для подсчета частоты слов, отсутствующих в словаре mystem\n",
    "    non_mystem_words = defaultdict(int)\n",
    "\n",
    "    # Обрабатываем каждое слово из результатов анализа\n",
    "    for word_info in parsed_data:\n",
    "        # Получаем словоформу\n",
    "        wordform = word_info['wordform']\n",
    "\n",
    "        # Проверяем, содержит ли элемент хотя бы одну букву или цифру\n",
    "        if not re.search(r'[а-яА-ЯёЁa-zA-Z0-9]', wordform):\n",
    "            continue\n",
    "\n",
    "        # Дополнительно проверяем, что элемент не состоит только из знаков препинания\n",
    "        if re.fullmatch(r'[^\\w\\s]', wordform):\n",
    "            continue\n",
    "\n",
    "        # Проверяем, отсутствует ли слово в словаре mystem\n",
    "        if not word_info['in_dictionary']:\n",
    "            # Нормализуем словоформу (приводим к нижнему регистру для объединения разных форм)\n",
    "            normalized_word = wordform.lower().strip()\n",
    "            # Увеличиваем счетчик частоты для данного слова\n",
    "            non_mystem_words[normalized_word] += 1\n",
    "\n",
    "    # Преобразуем defaultdict в обычный dict для возврата\n",
    "    return dict(non_mystem_words)\n",
    "\n",
    "def print_non_mystem_dict(non_mystem_dict):\n",
    "    if not non_mystem_dict:\n",
    "        print(\"Слова, отсутствующие в словаре mystem, не найдены.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{'Слово':<20} {'Частота'}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Сортируем по частоте (по убыванию)\n",
    "    sorted_words = sorted(non_mystem_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for word, frequency in sorted_words:\n",
    "        print(f\"{word:<20} {frequency}\")\n",
    "\n",
    "def save_non_mystem_dict(non_mystem_dict, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(non_mystem_dict, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Словарь успешно сохранен в файл: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении словаря: {e}\")\n",
    "\n",
    "\n",
    "# Анализ текста\n",
    "text = \"Этот стартап получил значительный фандрайзинг.\"\n",
    "parsed_data = parse_text(text)\n",
    "\n",
    "# Получение словаря слов, отсутствующих в mystem\n",
    "non_mystem_dict = get_non_mystem_dict(parsed_data)\n",
    "\n",
    "# Вывод результатов\n",
    "print_non_mystem_dict(non_mystem_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d5fc1",
   "metadata": {},
   "source": [
    "**Задание 9.** Напишите функцию `save_non_mystem_dict()`, сохраняющую структуру данных, получаемую функцией `get_non_mystem_dict()`, в текстовый файл формата TSV (tab-separated values). Слова в файле должны быть упорядочены по убыванию частоты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e29ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6945245",
   "metadata": {},
   "source": [
    "**Задание 10.** Напишите функцию `get_pos_distribution()`, на вход которой поступает словарь, формируемый функцией `get_dictionary()`, а на выходе выдается структура данных, содержащая частотное распределение частей речи в словаре со следующими значениями\n",
    "\n",
    "\n",
    "|часть речи|количество уникальных слов|общее количество слов|\n",
    "| -------- | ------------------------ | ------------------- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "693be778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e4f01",
   "metadata": {},
   "source": [
    "**Задание 11.** Проведите эксперименты с разработанными функциями:\n",
    "- скачайте 10 файлов с текстами разных жанров и разного размера (например, произведения классиков, современных писателей, новостные статьи, научные статьи и т.п.). *Учитывайте кодировку* – все файлы должны быть в UTF-8;\n",
    "- обработайте файлы при помощи функций `parse_text()`, `get_dictionary()` и `get_non_mystem_dict()`, и сохраните результаты в текстовых файлах при помощи функций `save_morph_results()`, `save_dictionary()` и `save_non_mystem_dict()`. Измеряйте время запуска функций! (см. следующий пункт);\n",
    "- заполните следующую таблицу:\n",
    "\n",
    "|Файл|Размер, байт|Размер текста (кол-во слов)|Размер словаря (кол-во уникальных слов)|Время работы get_dictionary(), сек.|\n",
    "|----|------------|---------------------------|---------------------------------------|-----------------------------------|\n",
    "- для самого большого словаря постройте частотное распределение слов:\n",
    "  - по оси ординат – частота,\n",
    "  - по оси абсцисс – слова, упорядоченные по убыванию частоты (по-другому, ранги слов);\n",
    "- постройте график зависимость времени морфологического анализа от размера текстового файла;\n",
    "- распределение частей речи, полученное функцией `get_pos_distribution()`, выведите на экран в виде таблицы и графика.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e8c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
